---
title: "Prediction Project"
author: "~ M.A.N.D.A.R ~"
date: "26-JAN-2021"
output: html_document
---

```{r setup, include=FALSE}
library(caret)
library(rattle)
library(rpart)
library(randomForest)
library(gbm)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)

```

## Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

The goal of this project is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 

More information is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

### Data

The training data for this project are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har

## Executive Summary

This report describes different methods used to build models, how they are cross validated, what is the accuracy of each of the model, and which is the best model. Here, we tried 3 different models. We first used the training data to build the models, and then used the models to predict using the test data and identified the Accuracy in each of the model. After comparing all the 3 models, we then concluded the best model with evidence (with respect to Accuracy).

## Data Modeling

The first step before building data models is eye-balling the structure of the datasets and cleaning the data as required. A cleaned data always helps in building better models. Let's analyze and prepare the datasets before we build the models.

### Data Preparation

Firstly, load the Training and Testing datasets from the given URLs and check their dimensions and structure of training data (shown in *Appendix-1*)

```{r Prep1}
# Load the training data and check its dimensions
TrainData <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"), header=TRUE)
dim(TrainData)

# Load the training data and check its dimensions
TestData <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"), header=TRUE)
dim(TestData)
```

We can see that the training data set is made of 19,622 observations (rows) on 160 variables (columns). From the structure of the training data in *Appendix-1* we can also notice that many columns have NA or blank values. Thus, we need to remove them to use cleaner data for building model(s). So, let's get the indexes of the columns having at least 90% of NA or blank values on the training dataset

```{r Prep2}
indColToRemove <- which(colSums(is.na(TrainData) | TrainData == "") > 0.9*dim(TrainData)[1]) 
TrainDataClean <- TrainData[,-indColToRemove]
```

We can also see that first seven columns of the dataset contains information about the people carrying out the activities and timestamp of their activities etc. Since they do not impact Classe variable, these can be removed from the training data to be used to build our model.

```{r Prep3}
TrainDataClean <- TrainDataClean[,-c(1:7)]
dim(TrainDataClean)

```

After cleaning, the new training data set has only 53 columns.
This is now a cleaner training dataset that can be use to build our model.
But before that, let's do the same cleanup to the testing data as well.

```{r Prep4}
indColToRemove <- which(colSums(is.na(TestData) |TestData=="")>0.9*dim(TestData)[1]) 
TestDataClean <- TestData[,-indColToRemove]
TestDataClean <- TestDataClean[,-1]
dim(TestDataClean)
```

Cleaned testing data also reduced the number of columns to 59. We are now good and now ready to build the models.

### Building the models

The goal of this project is to predict the manner in which the participants did the exercise. This is the "classe" variable in the training set, against which the models to be built.

Let's partition the cleaned training data to have 75% of the data to be used towards building training models.
```{r mod1}
# Here we create a partition of the traning data set 
set.seed(12345)
inTrain1 <- createDataPartition(TrainDataClean$classe, p=0.75, list=FALSE)
Train1 <- TrainDataClean[inTrain1,]
Test1 <- TrainDataClean[-inTrain1,]
dim(Train1)
```

Let's now models using 3 different methods: 

* Classification tree 
* Random forest 
* Gradient boosting method

Details of the 3 models can be found in *Appendix-2*

**1. Classification Tree**

For plotting a Classification Tree, let's train the data using 5 folds. Usually, 5 or 10 folds can be used, but 10 folds gives higher run times with no significant increase of the accuracy.

Once done, we will draw the Classification Tree.

```{r CT1}
trControl <- trainControl(method = "cv", number = 5)

model_CT <- train(classe ~ ., data=Train1, method="rpart", trControl=trControl)

fancyRpartPlot(model_CT$finalModel)

```

Let's now check the accuracy of the model.

In order to limit the effects of overfitting, and improve the efficicency of the models, we will use the *cross-validation technique. 

```{r CT2}
trainpred <- predict(model_CT,newdata=Test1)

confMatCT <- confusionMatrix(Test1$classe,trainpred)

# display confusion matrix and model accuracy

confMatCT$table         # Confusion Matrix

confMatCT$overall[1]    # Accuracy

```

We can notice that the accuracy of this first model is very low (about 49%). This means that the outcome class will not be predicted very well by the other predictors.


**2. Random Forest**

Let's create a model using Random forest technique and plot the Accuracy of the model against predictors.
```{r RF1}

model_RF <- train(classe~., data=Train1, method="rf", trControl=trControl, verbose=FALSE)

```

Let's now check the accuracy of the model.

```{r RF2}

plot(model_RF, main="Accuracy of Random forest model by number of predictors")

trainpred <- predict(model_RF,newdata=Test1)

confMatRF <- confusionMatrix(Test1$classe,trainpred)

# display confusion matrix and model accuracy

confMatRF$table         # Confusion Matrix

confMatRF$overall[1]    # Accuracy

```

With random forest, we reach an accuracy of 99.4% using cross-validation with 5 steps.

It is also important to find out the optimal number of predictors that are significant. Let's find out the important variables.

```{r RF3}
MostImpVars <- varImp(model_RF)
MostImpVars
```

Here we can notice that the most number of predictors out of total 52 are only 20. 

Let's also find out the model error of the Random forest model.

```{r RF4}
plot(model_RF$finalModel,main="Model error of Random forest model by number of trees")

```

With the plot, we can concluse that there is no significal increase of the accuracy with more predictors, and the slope decreases significantly after the initial important predictors, even if the accuracy is still very good. The fact that not all the accuracy is worse with all the available predictors lets us suggest that there may be some dependencies between them.


**3. Gradient Boosting Method**

Let's create a model using Gradient Boosting Model and plot the Accuracy of the model against predictors.
```{r GBM1}

model_GBM <- train(classe~., data=Train1, method="gbm", trControl=trControl, verbose=FALSE)

plot(model_GBM)
```

Let's now check the accuracy of the model.

```{r GBM2}
trainpred <- predict(model_GBM,newdata=Test1)

confMatGBM <- confusionMatrix(Test1$classe,trainpred)

confMatGBM$table        # Confusion Matrix

confMatGBM$overall[1]   # Accuracy

```

We can see that the Accuracy of the Gradient Boosting model increases with increase in Tree depth and  Accuracy with 5 folds is little over 96%.

### Decision

This shows that the random forest model is the best one. We will then use it to predict the values of classe for the test data set.

Let's run the best model on the testing data of 20 variables, and find out the output.

```{R decision}
Result <- predict(model_RF, newdata=TestDataClean)
print(Result)
```

## Conclusion

The goal of this project is to predict the manner in which the participants did the exercise. This is the "classe" variable in the training set, against which 3 models were built.

* Classification Tree - Accuracy: 48.78%
* Random Forest - Accuracy: 99.39%
* Gradient Boost - Accurtacy: 96.04%

We can conclude that the Random Forest model fitted the best.


## Appendix

### Appendix-1: Structure of Training Data
```{r App1}
str(TrainData)
```

### Appendix-2: Details of the 3 models

**1. Classification Tree**
```{r App21}
print(model_CT)
```

**2. Random Forest**
```{r App22}
print(model_RF)
```

**3. Gradient Boosting Method**
```{r App23}
print(model_GBM)

```
